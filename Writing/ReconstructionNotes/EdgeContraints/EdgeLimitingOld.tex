\documentclass[10pt]{article}

  \usepackage{pgfplots}
\pgfplotsset{compat=newest}
%% the following commands are needed for some matlab2tikz features
\usetikzlibrary{plotmarks}
\usetikzlibrary{arrows.meta}
\usepgfplotslibrary{patchplots}
\usepackage{grffile}
\usepackage{amsmath}
\usepackage{lineno}


%\usepackage{fullpage}
\usepackage[top=1in, bottom=1in, left=0.8in, right=1in]{geometry}
\usepackage{multicol}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{graphicx,psfrag}
\usepackage[pdf]{pstricks}

\definecolor{lightblue}{rgb}{.80,.9,1}
\newcommand{\hl}[1]
    {\par\colorbox{lightblue}{\parbox{\linewidth}{#1}}}

\newcommand{\defn}{\stackrel{\textrm{\scriptsize def}}{=}}

\setlength{\columnsep}{0.1pc}

\title{Montonicity Preserving Polynomial Reconstruction}
\author{Jordan Pitt -- \texttt{jordan.pitt@anu.edu.au}}

% TIME ON EVERY PAGE AS WELL AS THE FILE NAME
\usepackage{fancyhdr}
\usepackage{currfile}
\usepackage[us,12hr]{datetime} % `us' makes \today behave as usual in TeX/LaTeX
\fancypagestyle{plain}{
\fancyhf{}
\rfoot{\emph{\footnotesize \textcopyright  Serre Notes by  J. Pitt, C. Zoppou and S. Roberts.}
 \\ File Name: {\currfilename} \\ Date: {\ddmmyyyydate\today} at \currenttime}
\lfoot{Page \thepage}
\renewcommand{\headrulewidth}{0pt}}
\pagestyle{plain}

\definecolor{mycolor1}{rgb}{0.00000,0.44700,0.74100}%
\definecolor{mycolor2}{rgb}{0.85000,0.32500,0.09800}%
\definecolor{mycolor3}{rgb}{0.92900,0.69400,0.12500}%
\definecolor{mycolor4}{rgb}{0.49400,0.18400,0.55600}%
\definecolor{mycolor5}{rgb}{0.46600,0.67400,0.18800}% 
\definecolor{mycolor6}{rgb}{0.30100,0.74500,0.93300}%
\definecolor{mycolor7}{rgb}{0.63500,0.07800,0.18400}%

\newcommand\minmod{\text{minmod}}  

\newcommand\T{\rule{0pt}{3ex }}       % Top table strut
\newcommand\B{\rule[-4ex]{0pt}{4ex }} % Bottom table strut

\newcommand\TM{\rule{0pt}{2.8ex }}       % Top matrix strut
\newcommand\BM{\rule[-2ex]{0pt}{2ex }} % Bottom matrix strut

\newcommand{\vecn}[1]{\boldsymbol{#1}}
\DeclareRobustCommand{\solidrule}[1][0.25cm]{\rule[0.5ex]{#1}{1.5pt}}

\DeclareRobustCommand{\dashedrule}{\mbox{%
		\solidrule[2mm]\hspace{2mm}\solidrule[2mm]}}

\DeclareRobustCommand{\tikzcircle}[1]{\tikz{\filldraw[#1] (0,0) circle (0.5ex);}}	
	
	
\DeclareRobustCommand{\squaret}[1]{\tikz{\draw[#1,thick] (0,0) rectangle (0.2cm,0.2cm);}}
\DeclareRobustCommand{\circlet}[1]{\tikz{\draw[#1,thick] (0,0) circle [radius=0.1cm];}}
\DeclareRobustCommand{\trianglet}[1]{\tikz{\draw[#1,thick] (0,0) --
		(0.25cm,0) -- (0.125cm,0.25cm) -- (0,0);}}
\DeclareRobustCommand{\crosst}[1]{\tikz{\draw[#1,thick] (0cm,0cm) --
		(0.1cm,0.1cm) -- (0cm,0.2cm) -- (0.1cm,0.1cm) -- (0.2cm,0.2cm) -- (0.1cm,0.1cm)-- (0.2cm,0cm);}}
\DeclareRobustCommand{\diamondt}[1]{\tikz{\draw[#1,thick] (0,0) --(0.1cm,0.15cm) -- (0.2cm,0cm) -- (0.1cm,-0.15cm) -- (0,0)  ;}}
\DeclareRobustCommand{\squareF}[1]{\tikz{\filldraw[#1,fill opacity= 0.3] (0,0) rectangle (0.2cm,0.2cm);}}

\begin{document}

\maketitle

\vspace{-0.3in}
\noindent
\rule{\linewidth}{0.4pt}

\section{Goal}
We are going to use the following notation for all polynomials
\[P^r_i(x) = a^{(r)}_r(x  - x_i)^r + a^{(r-1)}_r(x  - x_i)^{r-1} + \dots + a^{(0)}_r\]

We want to generate a reconstruction that maintains monotonicity (when considered as a function over the whole domain) and can achieve sufficient order when things are smooth.

In the smoothcase it would be sufficient to ensure that the derivative of the reconstruction matches the monotonicity in the region (see for instance monotonicity cubic spline interpolation).

However, in the discontinuous case we also have to restrict this even more, we require both a restriction on the derivatives for the piecewise polynomials and a condition across the edges to match the monotonicity. The derivative condition is no longer enough since  the discontinuity can break the monotonicity overall while maintaining the correct derivative. 

\subsection{Jumping Off Point - Slope Limited Linear Reconstruction}
The case of linears is a bit special it has some nice features working in its favour \\
1. Can think about interpolating between points \\
2. Most well studied.  \\

Point 1 is very important because it means that we can ensure monotonicity quite easily in the reconstruction all that is required is to always pick the minimum slope when all gradients agree, and 0 otherwise. 

The different gradients being:

\begin{align}
D^+ &= \dfrac{\bar{q}_{j+1} - \bar{q}_{j}}{\Delta x}  \\
D^c &= \dfrac{\bar{q}_{j+1} - \bar{q}_{j-1}}{2\Delta x}  \\
D^- &= \dfrac{\bar{q}_{j} - \bar{q}_{j-1}}{\Delta x}
\end{align}

Reuslting in the following reconstructions
\begin{align}
P_1^+ &= D^+ \left(x - x_j\right) + \bar{q}_{j} \\
P_1^c &= D^c \left(x - x_j\right) + \bar{q}_{j} \\
P_1^- &=D^- \left(x - x_j\right) + \bar{q}_{j}1
\end{align}
This formulations can be thought of in a couple different ways: \\
1. As linears maintaining the appropriate cell average values \\ 
2. By approximations to the derivatives appropriately added to the constant term \\
3. By the interpolation between 2 midpoints (j+1,j) , (j-1,j) with the middle one being an average of both $D^c = 0.5\left(D^+ + D^-\right)$. \\

These naive reconstructions are then combined in many different ways, the one of interest for us is the generalised minmod limiter
Which calculate the reconstruction slope $R$ as
\begin{align}
R = \text{minmod}\left( \theta D^+,D^c,\theta D^-\right)
\end{align}

\begin{align}
R = \text{minmod}\left( \theta D^+,0.5\left(D^+ + D^-\right),\theta D^-\right)
\end{align}

with $\theta \in \left[1,2\right]$. \\

When the set of points is monotone, all these derivative approximations will have the same sign and then the minmod limiter picks the one with the smallest absolute gradient. Since these gradients can be thought of as connecting cell centres, this ensures both the : \\

1. Derivative constraints - matching sign with monotonicity \\
2. Jump conditions since we choose the minimum gradient approximation of a collection of lines that contain the line that connects the cell centers (when theta = 1, this is obvious, different values will require a bit more work), the value at the edges will maintain the necessary jump condition.

\subsubsection{Proof of points }
WLOG lets assume that we have decreasing monotonicity so that $\bar{q}_{i-1} \ge \bar{q}_{i} \ge \bar{q}_{i+1}$. Then we have that

Point 1:
\begin{align}
D^+ &= \dfrac{\bar{q}_{j+1} - \bar{q}_{j}}{\Delta x}\le 0 \\
D^c &= \dfrac{\bar{q}_{j+1} - \bar{q}_{j-1}}{2\Delta x} \le 0 \\
D^- &= \dfrac{\bar{q}_{j} - \bar{q}_{j-1}}{\Delta x}  \le 0
\end{align}

Therefore we have that 

\begin{align}
R = \text{minmod}\left( \theta D^+,0.5\left(D^+ + D^-\right),\theta D^-\right) \le 0
\end{align}

So over the $j$th cell point 1 will hold - the derivative of the reconstructed polynomial is monotonic over the $j^th$ cell. Since this holds for all $j$ we can see that derivatives will respect the monotonicity over cells. \\

Point 2:
We need to show that the jump conditions satisfy the monotonicty as well. 
\[q^-_{j+1/2} \ge q^+_{j+1/2} \]
\[q^-_{j-1/2} \ge q^+_{j-1/2} \]

with 
\[q^-_{j+1/2} = R \left(x_{j+1/2} - x_{j}\right) + \bar{q}_{j} \]
\[q^+_{j-1/2} = R \left(x_{j-1/2} - x_{j}\right) + \bar{q}_{j} \]

Since $R$ uses the minmod function we have
	 
\begin{align}
\theta D^- \le R  \le 0 \\
\theta D^+ \le R  \le 0
\end{align}

at the left cell edge $x-x_j = -dx/2$ so we have
\[\theta D^- \left(- \Delta x/2\right) \ge R \left(-\Delta x/2\right)  \ge 0 \]
add $\bar{q}_{j}$
\[\theta D^- \left(-\Delta x/2\right) \ge R \left(-\Delta x/2\right)  \ge 0 \]
\[\theta D^- \left(-\Delta x/2\right) + \bar{q}_{j} \ge R \left(-\Delta x/2\right) + \bar{q}_{j}  \ge \bar{q}_{j} \]
\[\theta \dfrac{\bar{q}_{j} - \bar{q}_{j-1}}{\Delta x} \left(-\Delta x/2\right) + \bar{q}_{j} \ge R \left(-\Delta x/2\right) + \bar{q}_{j}  \ge \bar{q}_{j} \]

\[\frac{\theta}{2} \left(\bar{q}_{j-1} - \bar{q}_{j}  \right) + \bar{q}_{j} \ge R \left(-\Delta x/2\right) + \bar{q}_{j}  \ge \bar{q}_{j} \]

\[\frac{\theta}{2} \bar{q}_{j-1} - \frac{\theta}{2}\bar{q}_{j} + \frac{2}{2} \bar{q}_{j} \ge R \left(-\Delta x/2\right) + \bar{q}_{j}  \ge \bar{q}_{j} \]

\[\frac{\theta}{2} \bar{q}_{j-1} + \frac{2 - \theta}{2}\bar{q}_{j}  \ge R \left(-\Delta x/2\right) + \bar{q}_{j}  \ge \bar{q}_{j} \]

\[\frac{\theta}{2} \bar{q}_{j-1} + \frac{2 - \theta}{2}\bar{q}_{j}  \ge q^+_{j-1/2} \ge \bar{q}_{j} \]

%Since $\frac{2}{\theta} + \frac{\theta - 2}{\theta} = 1 $ and $ 0 \le \frac{2}{\theta} \le 1$ $ 0 \le \frac{\theta - 2}{\theta} \le 1$ this is a convex combination.  Since $\bar{q}_{j} \le \bar{q}_{j-1}$ we have
%\[ \frac{2}{\theta} \bar{q}_{j-1} + \frac{\theta - 2}{\theta}\bar{q}_{j-1} = \bar{q}_{j-1} \ge \frac{2}{\theta} \bar{q}_{j-1} + \frac{\theta - 2}{\theta}\bar{q}_{j}  \ge R \left(-\Delta x/2\right) + \bar{q}_{j}  \ge \bar{q}_{j} \]
%
%So we have that - which is quite a weak condition, but we can do better, this does show that the reconstruction preserves monotonicity between, but we need a jump condition.
%\[\bar{q}_{j-1} \ge  q^+_{j - 1/2}  \ge \bar{q}_{j} \]

We can also do more - lets use subscripts on gradient reconstructions. So we have
\[\theta D^-_j \left(-\Delta x/2\right) \ge R_j \left(-\Delta x/2\right)  \ge 0 \]

for the $j-1$ we have
\[\theta D^+_{j-1} \le R_{j-1} \le 0\]

Which on multiplication by $\Delta x/2$ to get the right edge becomes
\[\theta D^+_{j-1} \left(\Delta x / 2\right)  \le R_{j-1} \left(\Delta x / 2\right) \le 0\]
\[\theta D^+_{j-1} \left(\Delta x / 2\right) + \bar{q}_{j-1}  \le R_{j-1} \left(\Delta x / 2\right) + \bar{q}_{j-1}  \le  \bar{q}_{j-1} \]

Reversing  so get same direction as before
\[\bar{q}_{j-1} \ge R_{j-1} \left(\Delta x / 2\right) + \bar{q}_{j-1} \ge \theta D^+_{j-1} \left(\Delta x / 2\right) + \bar{q}_{j-1}    \]

\[\bar{q}_{j-1} \ge q^-_{j-1/2} \ge \theta D^+_{j-1} \left(\Delta x / 2\right) + \bar{q}_{j-1}    \]

Expanding $D^+_{j-1}$

\[\bar{q}_{j-1} \ge q^-_{j-1/2} \ge \theta \dfrac{\bar{q}_{j} - \bar{q}_{j-1}}{\Delta x} \left(\Delta x / 2\right) + \bar{q}_{j-1}    \]

\[\bar{q}_{j-1} \ge q^-_{j-1/2} \ge  \frac{\theta}{2} \left(\bar{q}_{j} - \bar{q}_{j-1}\right) + \bar{q}_{j-1}    \]

\[\bar{q}_{j-1} \ge q^-_{j-1/2} \ge  \frac{\theta}{2} \bar{q}_{j}  - \frac{\theta}{2}\bar{q}_{j-1} + \bar{q}_{j-1}    \]

\[\bar{q}_{j-1} \ge q^-_{j-1/2} \ge \frac{2 - \theta}{2}\bar{q}_{j-1} +  \frac{\theta}{2} \bar{q}_{j}   \]

Ok lets put these two chains together
\begin{align}
\frac{\theta}{2} \bar{q}_{j-1} + \frac{2 - \theta}{2}\bar{q}_{j}  \ge q^+_{j-1/2} \ge \bar{q}_{j} \\
\bar{q}_{j-1} \ge q^-_{j-1/2} \ge \frac{2 - \theta}{2}\bar{q}_{j-1} +  \frac{\theta}{2} \bar{q}_{j} 
\end{align}


Need to show that
\[\frac{2 - \theta}{2}\bar{q}_{j-1} +  \frac{\theta}{2} \bar{q}_{j}  \ge \frac{\theta}{2} \bar{q}_{j-1} + \frac{2 - \theta}{2}\bar{q}_{j} \]

To tie these two together and obtain our result. 


%We had
%\[\bar{q}_{j-1} \ge R_{j-1} \left(\Delta x / 2\right) + \bar{q}_{j-1} \ge \theta D^+_{j-1} \left(\Delta x / 2\right) + \bar{q}_{j-1}    \]
%
%since $\bar{q}_{j-1}  \ge \bar{q}_{j} $ So we have
%\[\theta D^+_{j-1} \left(\Delta x / 2\right) + \bar{q}_{j-1} \ge \theta D^+_{j-1} \left(\Delta x / 2\right) + \bar{q}_{j}\]
%\[\theta \dfrac{\bar{q}_{j} - \bar{q}_{j-1}}{\Delta x} \left(\Delta x / 2\right) + \bar{q}_{j-1} \ge \theta \dfrac{\bar{q}_{j} - \bar{q}_{j-1}}{\Delta x} \left(\Delta x / 2\right) + \bar{q}_{j}\]
%
%\[ \frac{\theta}{2} \left(\bar{q}_{j} - \bar{q}_{j-1}\right) + \bar{q}_{j-1} \ge \frac{\theta}{2} \left(\bar{q}_{j} - \bar{q}_{j-1}\right)  + \bar{q}_{j}\]
%
%\[ \frac{\theta}{2}\bar{q}_{j} -  \frac{\theta}{2}\bar{q}_{j-1} + \bar{q}_{j-1} \ge  \frac{\theta}{2} \bar{q}_{j} -  \frac{\theta}{2} \bar{q}_{j-1} + \bar{q}_{j}\]
%
%\[   \frac{2 - \theta }{2}\bar{q}_{j-1} + \frac{\theta}{2}\bar{q}_{j} \ge  \frac{2 + \theta}{2} \bar{q}_{j} -  \frac{\theta}{2} \bar{q}_{j-1} \]




%We also have $\theta \in \left[1,2\right]$. Thus we have that
%\[2 \ge \theta \ge 1\]
%
%so we have
%\[1 \ge \frac{\theta}{2} \ge \frac{1}{2}\]
%
%Since $\frac{2 - \theta}{2} = 1 - \frac{\theta}{2} $
% We have that
% \[- \frac{1}{2} \ge -\frac{\theta}{2} \ge -1 \]
% \[1 - \frac{1}{2} \ge 1-\frac{\theta}{2} \ge 0 \]
%  \[\frac{1}{2} \ge \frac{2 - \theta}{2} \ge 0 \]
%  
%  So we have
%  \[1 \ge \frac{\theta}{2} \ge \frac{1}{2} \ge \frac{2 - \theta}{2} \ge 0 \]

Another approach
\[\theta D^-_{j} \le R_j \le 0\]
\[\theta D^-_{j} \le R_{j-1} \le 0\]

So we have that since $\theta D^-_{j}  + R_{j-1} \le R_j + R_{j-1} \le 0$ and  $\theta D^-_{j} + \theta D^-_{j} \le R_{j-1} + \theta D^-_{j} \le \theta D^-_{j}$
\[\theta D^-_{j} + \theta D^-_{j} \le R_j + R_{j-1} \le 0 \]

\[ \theta \Delta x D^-_{j} \le \left(R_j + R_{j-1}\right) \dfrac{\Delta x}{2}  \le 0 \]

\[ \theta \left(\bar{q}_{j} - \bar{q}_{j-1}\right) \le \left(R_j + R_{j-1}\right) \dfrac{\Delta x}{2}  \le 0 \]

\[ \theta \left(\bar{q}_{j} - \bar{q}_{j-1}\right) \le \left(R_j + R_{j-1}\right) \dfrac{\Delta x}{2}  \le 0 \]

\[ \theta\bar{q}_{j}  \le \left(R_j + R_{j-1}\right) \dfrac{\Delta x}{2} + \theta\bar{q}_{j-1}  \le \theta \bar{q}_{j-1} \]

\[  R_j \left( \dfrac{-\Delta x}{2}\right)  + \theta\bar{q}_{j}  \le R_{j-1} \left(\dfrac{\Delta x}{2}\right) + \theta\bar{q}_{j-1}  \le \theta\bar{q}_{j-1} \]

add $\left(1 - \theta\right)\bar{q}_{j-1}$

\[  R_j \left( \dfrac{-\Delta x}{2}\right)  + \theta\bar{q}_{j} + \left(1 - \theta\right)\bar{q}_{j-1}  \le R_{j-1} \left(\dfrac{\Delta x}{2}\right) + \bar{q}_{j-1}  \le \bar{q}_{j-1} \]

\[  R_j \left( \dfrac{-\Delta x}{2}\right)  + \bar{q}_{j-1}  + \theta\left(\bar{q}_{j} - \bar{q}_{j-1} \right) \le R_{j-1} \left(\dfrac{\Delta x}{2}\right) + \bar{q}_{j-1}  \le \bar{q}_{j-1} \]
 
\end{document} 